{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf8d70d-a87f-4cce-87b9-99effd7c2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comando per eseguire training (attualmente settato per i bicchieri della cartella \"glasses_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd743262-08ac-4a2d-9038-c84eb8ee9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-05-27 08:27:09.786000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748334429.804088    5340 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748334429.810070    5340 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "05/27/2025 08:27:13 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'timestep_spacing', 'variance_type', 'thresholding', 'dynamic_thresholding_ratio', 'sample_max_value', 'rescale_betas_zero_snr', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'use_post_quant_conv', 'latents_mean', 'latents_std', 'force_upcast', 'scaling_factor', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
      "05/27/2025 08:27:15 - INFO - __main__ - Converting the default unet to control-lora-v3 compatible unet\n",
      "{'time_embedding_act_fn', 'resnet_time_scale_shift', 'reverse_transformer_layers_per_block', 'upcast_attention', 'encoder_hid_dim_type', 'addition_time_embed_dim', 'projection_class_embeddings_input_dim', 'addition_embed_type_num_heads', 'extra_condition_names', 'mid_block_type', 'dual_cross_attention', 'cross_attention_norm', 'class_embed_type', 'num_class_embeds', 'time_embedding_dim', 'num_attention_heads', 'class_embeddings_concat', 'time_embedding_type', 'addition_embed_type', 'conv_in_kernel', 'dropout', 'use_linear_projection', 'conv_out_kernel', 'transformer_layers_per_block', 'encoder_hid_dim', 'attention_type', 'resnet_skip_time_act', 'mid_block_only_cross_attention', 'resnet_out_scale_factor', 'only_cross_attention', 'timestep_post_act', 'time_cond_proj_dim'} was not found in config. Values will be initialized to default values.\n",
      "Map: 100%|██████████████████████████| 193/193 [00:00<00:00, 14111.40 examples/s]\n",
      "05/27/2025 08:27:20 - INFO - __main__ - ***** Running training *****\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Num examples = 193\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Num batches each epoch = 49\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Num Epochs = 205\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Instantaneous batch size per device = 4\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "05/27/2025 08:27:20 - INFO - __main__ -   Total optimization steps = 10000\n",
      "Steps:  10%|▋      | 1000/10000 [04:30<38:53,  3.86it/s, loss=0.0145, lr=0.0001]05/27/2025 08:31:51 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-1000\n",
      "Model weights saved in out/lora-glasses/checkpoint-1000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:31:51 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-1000/optimizer.bin\n",
      "05/27/2025 08:31:51 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-1000/scheduler.bin\n",
      "05/27/2025 08:31:51 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-1000/sampler.bin\n",
      "05/27/2025 08:31:51 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-1000/scaler.pt\n",
      "05/27/2025 08:31:51 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-1000/random_states_0.pkl\n",
      "05/27/2025 08:31:51 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-1000\n",
      "Steps:  20%|█▍     | 2000/10000 [08:58<34:30,  3.86it/s, loss=0.0523, lr=0.0001]05/27/2025 08:36:18 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-2000\n",
      "Model weights saved in out/lora-glasses/checkpoint-2000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:36:19 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-2000/optimizer.bin\n",
      "05/27/2025 08:36:19 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-2000/scheduler.bin\n",
      "05/27/2025 08:36:19 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-2000/sampler.bin\n",
      "05/27/2025 08:36:19 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-2000/scaler.pt\n",
      "05/27/2025 08:36:19 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-2000/random_states_0.pkl\n",
      "05/27/2025 08:36:19 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-2000\n",
      "Steps:  30%|██▍     | 3000/10000 [13:26<30:37,  3.81it/s, loss=0.013, lr=0.0001]05/27/2025 08:40:47 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-3000\n",
      "Model weights saved in out/lora-glasses/checkpoint-3000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:40:47 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-3000/optimizer.bin\n",
      "05/27/2025 08:40:47 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-3000/scheduler.bin\n",
      "05/27/2025 08:40:47 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-3000/sampler.bin\n",
      "05/27/2025 08:40:47 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-3000/scaler.pt\n",
      "05/27/2025 08:40:47 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-3000/random_states_0.pkl\n",
      "05/27/2025 08:40:47 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-3000\n",
      "Steps:  40%|███▏    | 4000/10000 [17:54<25:54,  3.86it/s, loss=0.027, lr=0.0001]05/27/2025 08:45:15 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-4000\n",
      "Model weights saved in out/lora-glasses/checkpoint-4000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:45:15 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-4000/optimizer.bin\n",
      "05/27/2025 08:45:15 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-4000/scheduler.bin\n",
      "05/27/2025 08:45:15 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-4000/sampler.bin\n",
      "05/27/2025 08:45:15 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-4000/scaler.pt\n",
      "05/27/2025 08:45:15 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-4000/random_states_0.pkl\n",
      "05/27/2025 08:45:15 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-4000\n",
      "Steps:  50%|███▌   | 5000/10000 [22:23<29:13,  2.85it/s, loss=0.0116, lr=0.0001]05/27/2025 08:49:44 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-5000\n",
      "Model weights saved in out/lora-glasses/checkpoint-5000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:49:44 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-5000/optimizer.bin\n",
      "05/27/2025 08:49:44 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-5000/scheduler.bin\n",
      "05/27/2025 08:49:44 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-5000/sampler.bin\n",
      "05/27/2025 08:49:44 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-5000/scaler.pt\n",
      "05/27/2025 08:49:44 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-5000/random_states_0.pkl\n",
      "05/27/2025 08:49:44 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-5000\n",
      "Steps:  60%|███▌  | 6000/10000 [26:50<17:16,  3.86it/s, loss=0.00692, lr=0.0001]05/27/2025 08:54:11 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-6000\n",
      "Model weights saved in out/lora-glasses/checkpoint-6000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:54:12 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-6000/optimizer.bin\n",
      "05/27/2025 08:54:12 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-6000/scheduler.bin\n",
      "05/27/2025 08:54:12 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-6000/sampler.bin\n",
      "05/27/2025 08:54:12 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-6000/scaler.pt\n",
      "05/27/2025 08:54:12 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-6000/random_states_0.pkl\n",
      "05/27/2025 08:54:12 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-6000\n",
      "Steps:  70%|█████▌  | 7000/10000 [31:18<12:56,  3.87it/s, loss=0.055, lr=0.0001]05/27/2025 08:58:39 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-7000\n",
      "Model weights saved in out/lora-glasses/checkpoint-7000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 08:58:39 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-7000/optimizer.bin\n",
      "05/27/2025 08:58:39 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-7000/scheduler.bin\n",
      "05/27/2025 08:58:39 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-7000/sampler.bin\n",
      "05/27/2025 08:58:39 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-7000/scaler.pt\n",
      "05/27/2025 08:58:39 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-7000/random_states_0.pkl\n",
      "05/27/2025 08:58:39 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-7000\n",
      "Steps:  80%|█████▌ | 8000/10000 [35:47<08:40,  3.84it/s, loss=0.0477, lr=0.0001]05/27/2025 09:03:08 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-8000\n",
      "Model weights saved in out/lora-glasses/checkpoint-8000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 09:03:08 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-8000/optimizer.bin\n",
      "05/27/2025 09:03:08 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-8000/scheduler.bin\n",
      "05/27/2025 09:03:08 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-8000/sampler.bin\n",
      "05/27/2025 09:03:08 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-8000/scaler.pt\n",
      "05/27/2025 09:03:08 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-8000/random_states_0.pkl\n",
      "05/27/2025 09:03:08 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-8000\n",
      "Steps:  90%|██████▎| 9000/10000 [40:15<04:19,  3.86it/s, loss=0.0215, lr=0.0001]05/27/2025 09:07:36 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-9000\n",
      "Model weights saved in out/lora-glasses/checkpoint-9000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 09:07:36 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-9000/optimizer.bin\n",
      "05/27/2025 09:07:36 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-9000/scheduler.bin\n",
      "05/27/2025 09:07:36 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-9000/sampler.bin\n",
      "05/27/2025 09:07:36 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-9000/scaler.pt\n",
      "05/27/2025 09:07:36 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-9000/random_states_0.pkl\n",
      "05/27/2025 09:07:36 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-9000\n",
      "Steps: 100%|██████| 10000/10000 [44:43<00:00,  3.29it/s, loss=0.0137, lr=0.0001]05/27/2025 09:12:04 - INFO - accelerate.accelerator - Saving current state to out/lora-glasses/checkpoint-10000\n",
      "Model weights saved in out/lora-glasses/checkpoint-10000/pytorch_lora_weights.safetensors\n",
      "05/27/2025 09:12:04 - INFO - accelerate.checkpointing - Optimizer state saved in out/lora-glasses/checkpoint-10000/optimizer.bin\n",
      "05/27/2025 09:12:04 - INFO - accelerate.checkpointing - Scheduler state saved in out/lora-glasses/checkpoint-10000/scheduler.bin\n",
      "05/27/2025 09:12:04 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in out/lora-glasses/checkpoint-10000/sampler.bin\n",
      "05/27/2025 09:12:04 - INFO - accelerate.checkpointing - Gradient scaler state saved in out/lora-glasses/checkpoint-10000/scaler.pt\n",
      "05/27/2025 09:12:04 - INFO - accelerate.checkpointing - Random states saved in out/lora-glasses/checkpoint-10000/random_states_0.pkl\n",
      "05/27/2025 09:12:04 - INFO - __main__ - Saved state to out/lora-glasses/checkpoint-10000\n",
      "Model weights saved in out/lora-glasses/pytorch_lora_weights.safetensors=0.0001]\n",
      "Steps: 100%|█████| 10000/10000 [44:43<00:00,  3.73it/s, loss=0.00734, lr=0.0001]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train.py \\\n",
    "  --pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5 \\\n",
    "  --output_dir=out/lora-glasses \\\n",
    "  --dataset_script_path=exps/sd1_5_tile_pair_data.py \\\n",
    "  --conditioning_image_column=guide \\\n",
    "  --image_column=image \\\n",
    "  --caption_column=text \\\n",
    "  --lora_adapter_name=circles_adapter \\\n",
    "  --rank=16 \\\n",
    "  --init_lora_weights=gaussian \\\n",
    "  --loraplus_lr_ratio=1.0 \\\n",
    "  --resolution=512 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --train_batch_size=4 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --mixed_precision=fp16 \\\n",
    "  --max_train_steps=10000 \\\n",
    "  --checkpointing_steps=1000 \\\n",
    "  --validation_steps=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382d39b4-5616-4779-9f65-a5b085e399ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/progetto-tesi-control-lora-v3-main\n"
     ]
    }
   ],
   "source": [
    "cd ~/progetto-tesi-control-lora-v3-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa68eb8-c35c-418c-82bd-e7507db392f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
